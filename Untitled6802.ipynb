{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN54TjVu8VFbL7ShTikaICa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pscabral/Prezunic/blob/main/Untitled6802.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rClw6j02pCDj",
        "outputId": "11b39472-3e75-494c-bd4d-8eb0ada58353"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroceryStoreDataset'...\n",
            "remote: Enumerating objects: 6559, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293\u001b[K\n",
            "Receiving objects: 100% (6559/6559), 116.26 MiB | 46.94 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Especificar o caminho para o diretório que contém as imagens de bananas\n",
        "diretorio_imagens = '/content/GroceryStoreDataset/dataset/train/Fruit/Banana'\n",
        "\n",
        "# Listar todos os arquivos no diretório de imagens\n",
        "imagens = os.listdir(diretorio_imagens)\n",
        "\n",
        "# Pré-processamento e carregamento das imagens\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for imagem_nome in imagens:\n",
        "    # Construir o caminho completo para a imagem\n",
        "    caminho_imagem = os.path.join(diretorio_imagens, imagem_nome)\n",
        "\n",
        "    img = Image.open(caminho_imagem)\n",
        "    img = img.resize((224, 224))  # Redimensionar para o tamanho esperado pela VGG16\n",
        "    img = np.array(img)\n",
        "    img = preprocess_input(img)  # Pré-processamento específico da VGG16\n",
        "\n",
        "    X.append(img)\n",
        "    y.append(\"banana\")  # Agora, usamos \"banana\" como rótulo\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Codificar os rótulos em números\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Dividir o conjunto de dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Carregar o modelo VGG16 pré-treinado com pesos do ImageNet\n",
        "base_model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "# Adicionar camadas personalizadas para a tarefa de classificação\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "\n",
        "# Criar o modelo final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Descongelar algumas camadas convolucionais para ajuste fino\n",
        "for layer in base_model.layers[:15]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo com ajuste fino\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "accuracy = model.evaluate(X_test, y_test)[1]\n",
        "print(f'Acurácia no conjunto de teste: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Salvar o modelo no formato Keras nativo\n",
        "model.save(\"meu_modelo.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heLsg5C6z1XM",
        "outputId": "46263099-b966-4026-9691-523154731ff1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 41s 41s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 42s 42s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 41s 41s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "Acurácia no conjunto de teste: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Caminho para o diretório de imagens de treinamento\n",
        "diretorio_imagens_treinamento = '/content/GroceryStoreDataset/dataset/train/Fruit/Banana'\n",
        "\n",
        "# Caminho para o diretório de imagens de teste\n",
        "diretorio_imagens_teste = '/content/GroceryStoreDataset/dataset/test/Fruit/Banana'\n",
        "\n",
        "# Carregar o modelo VGG16 pré-treinado\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "# Função para realizar a classificação de imagem\n",
        "def classify_image(image_path, model):\n",
        "    # Carregar a imagem\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # Realizar a classificação de imagem\n",
        "    predictions = model.predict(image)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Lista de imagens no diretório de teste\n",
        "imagens_teste = os.listdir(diretorio_imagens_teste)\n",
        "\n",
        "# Suponhamos que você queira avaliar a precisão do modelo para a classe \"banana\"\n",
        "classe_alvo = \"banana\"\n",
        "precisoes = []\n",
        "\n",
        "# Loop através de todas as imagens de teste\n",
        "for imagem_teste in imagens_teste:\n",
        "    caminho_imagem_teste = os.path.join(diretorio_imagens_teste, imagem_teste)\n",
        "    predictions_teste = classify_image(caminho_imagem_teste, model)\n",
        "    decoded_predictions_teste = decode_predictions(predictions_teste, top=5)[0]\n",
        "\n",
        "    # Obter a classe com a maior confiança\n",
        "    class_id_teste, class_label_teste, class_score_teste = decoded_predictions_teste[0]\n",
        "\n",
        "    # Verificar se a classe predita corresponde à classe alvo\n",
        "    if class_label_teste.lower() == classe_alvo:\n",
        "        precisao = 1.0\n",
        "    else:\n",
        "        precisao = 0.0\n",
        "\n",
        "    precisoes.append(precisao)\n",
        "\n",
        "# Calcular a precisão média\n",
        "precisao_media = sum(precisoes) / len(precisoes)\n",
        "\n",
        "# Exibir o resultado da precisão média\n",
        "print(f'Precisão Média para a Classe \"{classe_alvo}\": {precisao_media:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfgG4HS81cU9",
        "outputId": "d74a9cd8-bd5a-4ab4-88b0-b0e71d318be5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 7s 0us/step\n",
            "1/1 [==============================] - 1s 527ms/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "1/1 [==============================] - 1s 586ms/step\n",
            "1/1 [==============================] - 1s 573ms/step\n",
            "1/1 [==============================] - 1s 523ms/step\n",
            "1/1 [==============================] - 0s 399ms/step\n",
            "1/1 [==============================] - 0s 409ms/step\n",
            "1/1 [==============================] - 0s 398ms/step\n",
            "1/1 [==============================] - 0s 402ms/step\n",
            "1/1 [==============================] - 0s 398ms/step\n",
            "1/1 [==============================] - 0s 408ms/step\n",
            "1/1 [==============================] - 0s 392ms/step\n",
            "1/1 [==============================] - 1s 707ms/step\n",
            "1/1 [==============================] - 1s 724ms/step\n",
            "1/1 [==============================] - 0s 467ms/step\n",
            "1/1 [==============================] - 0s 482ms/step\n",
            "1/1 [==============================] - 1s 529ms/step\n",
            "1/1 [==============================] - 1s 656ms/step\n",
            "1/1 [==============================] - 1s 571ms/step\n",
            "1/1 [==============================] - 1s 511ms/step\n",
            "1/1 [==============================] - 0s 402ms/step\n",
            "1/1 [==============================] - 0s 392ms/step\n",
            "1/1 [==============================] - 0s 388ms/step\n",
            "1/1 [==============================] - 0s 432ms/step\n",
            "1/1 [==============================] - 1s 526ms/step\n",
            "1/1 [==============================] - 1s 510ms/step\n",
            "1/1 [==============================] - 1s 578ms/step\n",
            "1/1 [==============================] - 1s 503ms/step\n",
            "1/1 [==============================] - 0s 402ms/step\n",
            "1/1 [==============================] - 0s 389ms/step\n",
            "1/1 [==============================] - 0s 388ms/step\n",
            "1/1 [==============================] - 0s 393ms/step\n",
            "1/1 [==============================] - 0s 392ms/step\n",
            "1/1 [==============================] - 0s 401ms/step\n",
            "1/1 [==============================] - 0s 394ms/step\n",
            "1/1 [==============================] - 0s 391ms/step\n",
            "1/1 [==============================] - 0s 402ms/step\n",
            "1/1 [==============================] - 0s 387ms/step\n",
            "1/1 [==============================] - 0s 398ms/step\n",
            "1/1 [==============================] - 0s 387ms/step\n",
            "1/1 [==============================] - 0s 404ms/step\n",
            "1/1 [==============================] - 0s 397ms/step\n",
            "1/1 [==============================] - 0s 393ms/step\n",
            "1/1 [==============================] - 0s 400ms/step\n",
            "1/1 [==============================] - 0s 393ms/step\n",
            "Precisão Média para a Classe \"banana\": 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from google.colab.patches import cv2_imshow\n",
        "from gtts import gTTS\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Caminho para a imagem de teste\n",
        "caminho_imagem_teste = '/content/GroceryStoreDataset/dataset/test/Fruit/Banana/Banana_001.jpg'  # Substitua pelo caminho da sua imagem de teste\n",
        "\n",
        "# Ler a imagem de teste\n",
        "image = cv2.imread(caminho_imagem_teste)\n",
        "\n",
        "# Redimensionar a imagem para o tamanho esperado pela VGG16\n",
        "image_resized = cv2.resize(image, (224, 224))\n",
        "\n",
        "# Adicionar a dimensão do lote (batch)\n",
        "image_resized = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "# Pré-processamento específico da VGG16\n",
        "image_preprocessed = preprocess_input(image_resized)\n",
        "\n",
        "# Fazer a previsão usando o modelo treinado\n",
        "predicted_label = model.predict(image_preprocessed)\n",
        "\n",
        "# Descodificar o rótulo previsto de volta para o nome da classe\n",
        "predicted_class = label_encoder.inverse_transform([np.argmax(predicted_label)])[0]\n",
        "\n",
        "# Exibir a imagem com a previsão\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "cv2.putText(image, f'{predicted_class}', (10, 50), font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Gerar o arquivo de áudio com o nome do produto previsto\n",
        "tts = gTTS(text=f'O produto é {predicted_class}', lang='pt')\n",
        "tts.save('/content/prevision.mp3')\n",
        "\n",
        "# Reproduzir o arquivo de áudio\n",
        "ipd.Audio('/content/prevision.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "uruThTIV5KIG",
        "outputId": "e06bd091-9279-4f72-a07d-9052c8a64afa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 445ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a8379d2b4305>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Descodificar o rótulo previsto de volta para o nome da classe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Exibir a imagem com a previsão\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y contains previously unseen labels: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [692]"
          ]
        }
      ]
    }
  ]
}